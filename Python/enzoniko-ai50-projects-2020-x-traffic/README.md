First of all, I experimented with 3 basic and necessary layers: A convolutional layer with RElu activation and 32 filters, a flattening layer (I noticed that without the flattening layer the code does not work, since it gives a shape incompatibility error) and a dense output layer with softmax activation. With those 3 basic layers and 10 epochs the results are not satisfying, the loss is almost 1.5 and the accuracy is less than 0.85, and the overfitting is a big problem here since the last results from the training set are considerably better.
With those bad results, I needed to try more layers so I started by adding a convolutional layer with RElu activation and 32 filters, and only with that change, the results are way better: The loss was 0.4 and the accuracy was 0.95 and we had less overfitting since the training and testing results had only a 0.2 difference. 
But I was not satisfied yet, so I added a max-pooling layer with a 3 by 3 pool between the two convolutional layers and this reduced the loss by 0.1, I noticed that if I added one more pooling layer the results were really bad, so I did not, instead, I removed the second convolutional layer and I added a hidden dense layer with NUM_CATEGORIES units and 0.5 dropout. I also tried to twist this values: Number and size of filters in the convolutional layer, pool size in the max-pooling layer, units in the hidden layer, and dropout, but it turns out that 32 filters with 3 by 3 size, 3 by 3 pool size, NUM_CATEGORIES units in the hidden layer and 0.5 dropout are the faster and most accurate options that I could achieve in the first day of testing. 
After all the testing of the second day, I arrived at the decision of keeping this layers: A convolutional layer that learns 32 filters using a 3x3 kernel and has Relu activation, a max-pooling layer with 3x3 pool size, a hidden layer with NUM_CATEGORIES units, Sigmoid activation and 0.2 dropout (this dropout configuration was the most accurate I could get), a flattening layer and an output layer with NUM_CATEGORIES units and Sigmoid activation.
With this configuration of layers the best results I could achieve were: 0.0025 of loss and 0.9915 of accuracy after 10 epochs each one taking 10 seconds to finish if I am not recording the screen. Also in the compiler, I used: Nadam as the compiler, Binary_crossentropy as the loss, and accuracy metrics.

Download the distribution code from https://cdn.cs50.net/ai/2020/x/projects/5/traffic.zip and unzip it.